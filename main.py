import os
import tempfile
from flask import Flask, request, send_from_directory, jsonify
from flask_cors import CORS
import openai
import requests
from pydub import AudioSegment
from werkzeug.utils import secure_filename

app = Flask(__name__)
CORS(app)
openai.api_key = os.environ.get("OPENAI_KEY")
ELEVENLABS_KEY = os.environ.get("ELEVENLABS_KEY")

FIELD_PROMPTS = {
    "Date": "ğŸ™ï¸ Ø£Ø±Ø³Ù„ ØªØ§Ø±ÙŠØ® Ø§Ù„ÙˆØ§Ù‚Ø¹Ø©.",
    "Briefing": "ğŸ™ï¸ Ø£Ø±Ø³Ù„ Ù…ÙˆØ¬Ø² Ø§Ù„ÙˆØ§Ù‚Ø¹Ø©.",
    "LocationObservations": "ğŸ™ï¸ Ø£Ø±Ø³Ù„ Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø­ÙŠØ« Ø¨Ù…Ø¹Ø§ÙŠÙ†Ø© Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø­Ø§Ø¯Ø« ØªØ¨ÙŠÙ† Ù…Ø§ ÙŠÙ„ÙŠ .....",
    "Examination": "ğŸ™ï¸ Ø£Ø±Ø³Ù„ Ù†ØªÙŠØ¬Ø© Ø§Ù„ÙØ­Øµ Ø§Ù„ÙÙ†ÙŠ ... Ø­ÙŠØ« Ø¨ÙØ­Øµ Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ø­Ø§Ø¯Ø« ØªØ¨ÙŠÙ† Ù…Ø§ ÙŠÙ„ÙŠ .....",
    "Outcomes": "ğŸ™ï¸ Ø£Ø±Ø³Ù„ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø­ÙŠØ« Ø£Ù†Ù‡ Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§ÙŠÙ†Ø© Ùˆ Ø£Ø¬Ø±Ø§Ø¡ Ø§Ù„ÙØ­ÙˆØµ Ø§Ù„ÙÙ†ÙŠØ© Ø§Ù„Ù„Ø§Ø²Ù…Ø© ØªØ¨ÙŠÙ† Ù…Ø§ ÙŠÙ„ÙŠ:.",
    "TechincalOpinion": "ğŸ™ï¸ Ø£Ø±Ø³Ù„ Ø§Ù„Ø±Ø£ÙŠ Ø§Ù„ÙÙ†ÙŠ."
}

@app.route("/")
def index():
    return send_from_directory("static", "index.html")

@app.route("/script.js")
def script():
    return send_from_directory("static", "script.js")

@app.route("/voice", methods=["POST"])
def handle_voice():
    file = request.files['audio']
    field = request.args.get("field") or ""
    filename = secure_filename(file.filename)

    with tempfile.NamedTemporaryFile(delete=False, suffix=".webm") as temp_webm:
        file.save(temp_webm.name)
        sound = AudioSegment.from_file(temp_webm.name)
        temp_wav = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
        sound.export(temp_wav.name, format="wav")

    with open(temp_wav.name, "rb") as audio_file:
        transcript = openai.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file,
            language="ar"
        ).text

    gpt_prompt = f"""
    Ø£Ø¹Ø¯ ØµÙŠØ§ØºØ© Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ù‡Ù†ÙŠØ© Ù„ØªØ¶Ù…ÙŠÙ†Ù‡ ÙÙŠ ØªÙ‚Ø±ÙŠØ± ÙÙ†ÙŠ Ù„Ù„Ø´Ø±Ø·Ø© Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù†Ù‰:

    "{transcript}"
    """
    completion = openai.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "user", "content": gpt_prompt}
        ]
    )
    rephrased = completion.choices[0].message.content.strip()

    # TTS with ElevenLabs
    tts_response = requests.post(
        "https://api.elevenlabs.io/v1/text-to-speech/AZnzlk1XvdvUeBnXmlld/stream",
        headers={
            "xi-api-key": ELEVENLABS_KEY,
            "Content-Type": "application/json"
        },
        json={
            "text": rephrased,
            "voice_settings": {"stability": 0.3, "similarity_boost": 0.8}
        }
    )

    audio_path = f"static/reply_{field}.mp3"
    with open(audio_path, "wb") as f:
        f.write(tts_response.content)

    return jsonify({
        "preview": rephrased,
        "audio_url": f"/reply_{field}.mp3"
    })

@app.route("/reply", methods=["POST"])
def handle_reply():
    file = request.files['audio']
    filename = secure_filename(file.filename)

    with tempfile.NamedTemporaryFile(delete=False, suffix=".webm") as temp_webm:
        file.save(temp_webm.name)
        sound = AudioSegment.from_file(temp_webm.name)
        temp_wav = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
        sound.export(temp_wav.name, format="wav")

    with open(temp_wav.name, "rb") as audio_file:
        transcript = openai.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file,
            language="ar"
        ).text.strip()

    classify_prompt = f"""
    Ù‡Ù„ Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© ØªØ¹Ù†ÙŠ Ù…ÙˆØ§ÙÙ‚Ø©ØŒ ØªØ¹Ø¯ÙŠÙ„ØŒ Ø£Ù… Ø±ÙØ¶ØŸ Ø§Ù„Ø¬ÙˆØ§Ø¨ ÙÙ‚Ø· Ø¨ÙƒÙ„Ù…Ø© ÙˆØ§Ø­Ø¯Ø©:
    Ø§Ù„Ø¬Ù…Ù„Ø©: "{transcript}"
    Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙÙ‚Ø·: Ù…ÙˆØ§ÙÙ‚Ø© Ø£Ùˆ ØªØ¹Ø¯ÙŠÙ„ Ø£Ùˆ Ø±ÙØ¶
    """
    classification = openai.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": classify_prompt}]
    ).choices[0].message.content.strip()

    if "Ù…ÙˆØ§ÙÙ‚Ø©" in classification:
        return jsonify({"action": "accept"})
    elif "Ø±ÙØ¶" in classification:
        return jsonify({"action": "redo"})
    elif "ØªØ¹Ø¯ÙŠÙ„" in classification or "Ø£Ø¶Ù" in transcript:
        edit_prompt = f"Ø¹Ø¯Ù„ Ø§Ù„Ù†Øµ Ø§Ù„Ø³Ø§Ø¨Ù‚ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„ØªÙˆØ¬ÙŠÙ‡: {transcript}"
        edited = openai.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": edit_prompt}]
        ).choices[0].message.content.strip()
        return jsonify({"action": "edit", "modified_text": edited})

    return jsonify({"action": "unknown"})

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", 10000)))
