import openai
print("âœ… OpenAI version:", openai.__version__)
import os
import base64
import tempfile
from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
from openai import OpenAI
from elevenlabs import Voice, VoiceSettings, set_api_key, generate

app = Flask(__name__)
CORS(app)

# Ø¥Ø¹Ø¯Ø§Ø¯ Ù…ÙØ§ØªÙŠØ­ API
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
set_api_key(os.getenv("ELEVENLABS_API_KEY"))

# Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
user_session = {
    "current_field": "Date",
    "fields": {},
}

# ØªØ±ØªÙŠØ¨ Ø§Ù„Ø­Ù‚ÙˆÙ„
field_prompts = {
    "Date": "ğŸ™ï¸ Ø£Ø®Ø¨Ø±Ù†ÙŠ Ø¹Ù† ØªØ§Ø±ÙŠØ® Ø§Ù„ÙˆØ§Ù‚Ø¹Ø©.",
    "Briefing": "ğŸ™ï¸ Ù…Ø§ Ù‡Ùˆ Ù…ÙˆØ¬Ø² Ø§Ù„ÙˆØ§Ù‚Ø¹Ø©ØŸ",
    "LocationObservations": "ğŸ™ï¸ Ù…Ø§Ø°Ø§ Ù„Ø§Ø­Ø¸Øª Ø¹Ù†Ø¯ Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŸ",
    "Examination": "ğŸ™ï¸ Ù…Ø§ Ù‡ÙŠ Ù†ØªÙŠØ¬Ø© Ø§Ù„ÙØ­Øµ Ø§Ù„ÙÙ†ÙŠØŸ",
    "Outcomes": "ğŸ™ï¸ Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„ÙØ­ØµØŸ",
    "TechincalOpinion": "ğŸ™ï¸ Ù…Ø§ Ù‡Ùˆ Ø±Ø£ÙŠÙƒ Ø§Ù„ÙÙ†ÙŠØŸ"
}
field_order = list(field_prompts.keys())

# Ù†Ù‚Ø·Ø© Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© (Ø±Ø³Ø§Ù„Ø© ØªØ±Ø­ÙŠØ¨ + Ø£ÙˆÙ„ Ø³Ø¤Ø§Ù„)
@app.route("/")
def home():
    return "ğŸ™ï¸ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„ØµÙˆØªÙŠ ÙŠØ¹Ù…Ù„!"

@app.route("/stream-audio")
def stream_audio():
    text = request.args.get("text", "")
    audio = generate(
        text=text,
        voice=Voice(
            voice_id="21m00Tcm4TlvDq8ikWAM",  # Rachel
            settings=VoiceSettings(stability=0.4, similarity_boost=0.85)
        ),
        model="eleven_multilingual_v2"
    )
    temp_path = tempfile.mktemp(suffix=".mp3")
    with open(temp_path, "wb") as f:
        f.write(audio)
    return send_file(temp_path, mimetype="audio/mpeg")

# Ø§Ø³ØªÙ„Ø§Ù… Ø§Ù„ØµÙˆØª Ù…Ù† Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©
@app.route("/submitAudio", methods=["POST"])
def submit_audio():
    data = request.get_json()
    audio_b64 = data.get("audio")
    if not audio_b64:
        return jsonify({
            "response": field_prompts[user_session["current_field"]],
            "action": "continue",
            "transcript": ""
        })

    # Ø­ÙØ¸ Ù…Ù„Ù Ø§Ù„ØµÙˆØª Ù…Ø¤Ù‚ØªØ§Ù‹
    audio_data = base64.b64decode(audio_b64.split(",")[1])
    with tempfile.NamedTemporaryFile(delete=False, suffix=".webm") as f:
        f.write(audio_data)
        f.flush()
        audio_path = f.name

    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ
    result = client.audio.transcriptions.create(
        model="whisper-1",
        file=open(audio_path, "rb"),
        language="ar"
    )
    transcript = result.text.strip()

    # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØµÙŠØ§ØºØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… GPT
    field = user_session["current_field"]
    gpt_result = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "Ø£Ø¹Ø¯ ØµÙŠØ§ØºØ© Ù‡Ø°Ù‡ Ø§Ù„Ø¬Ù…Ù„Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø´ÙƒÙ„ Ù…Ù‡Ù†ÙŠ Ù„ØªÙˆØ¶Ø¹ Ø¶Ù…Ù† ØªÙ‚Ø±ÙŠØ± Ø±Ø³Ù…ÙŠ:"},
            {"role": "user", "content": transcript}
        ]
    )
    rephrased = gpt_result.choices[0].message.content.strip()
    user_session["fields"][field] = rephrased

    # Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ø¥Ù„Ù‰ Ø§Ù„Ø­Ù‚Ù„ Ø§Ù„ØªØ§Ù„ÙŠ
    next_index = field_order.index(field) + 1
    if next_index < len(field_order):
        next_field = field_order[next_index]
        user_session["current_field"] = next_field
        response_text = field_prompts[next_field]
        return jsonify({
            "response": response_text,
            "action": "continue",
            "transcript": rephrased
        })
    else:
        return jsonify({
            "response": "âœ… ØªÙ… Ø§Ø³ØªÙ„Ø§Ù… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. ÙŠØªÙ… Ø§Ù„Ø¢Ù† ØªØ¬Ù‡ÙŠØ² Ø§Ù„ØªÙ‚Ø±ÙŠØ±.",
            "action": "done",
            "transcript": rephrased
        })

