import os
import tempfile
import base64
import ffmpeg
from flask import Flask, request, jsonify
from flask_cors import CORS
from openai import OpenAI
from elevenlabs import generate, Voice, VoiceSettings, set_api_key
from docxtpl import DocxTemplate

# Ø¥Ø¹Ø¯Ø§Ø¯ Flask
app = Flask(__name__)
CORS(app)

# Ù…ÙØ§ØªÙŠØ­ API
openai_key = os.getenv("OPENAI_API_KEY")
eleven_key = os.getenv("ELEVENLABS_API_KEY")
set_api_key(eleven_key)
client = OpenAI(api_key=openai_key)

# Ø§Ù„Ø¬Ù„Ø³Ø©
user_session = {
    "current_field": None,
    "fields": {},
    "last_prompt": "",
}

# Ø§Ù„Ø­Ù‚ÙˆÙ„ ÙˆØªØ¹Ø±ÙŠÙØ§ØªÙ‡Ø§
field_prompts = {
    "Date": "ğŸ™ï¸ Ù…Ø§ Ù‡Ùˆ ØªØ§Ø±ÙŠØ® Ø§Ù„ÙˆØ§Ù‚Ø¹Ø©ØŸ",
    "Briefing": "ğŸ™ï¸ Ø£Ø®Ø¨Ø±Ù†ÙŠ Ø¨Ù…ÙˆØ¬Ø² Ø§Ù„ÙˆØ§Ù‚Ø¹Ø©.",
    "LocationObservations": "ğŸ™ï¸ Ù…Ø§Ø°Ø§ Ù„Ø§Ø­Ø¸Øª Ø¹Ù†Ø¯ Ù…Ø¹Ø§ÙŠÙ†Ø© Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø­Ø§Ø¯Ø«ØŸ",
    "Examination": "ğŸ™ï¸ Ù…Ø§ Ù†ØªÙŠØ¬Ø© Ø§Ù„ÙØ­Øµ Ø§Ù„ÙÙ†ÙŠØŸ",
    "Outcomes": "ğŸ™ï¸ Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„ÙØ­ØµØŸ",
    "TechincalOpinion": "ğŸ™ï¸ Ù…Ø§ Ù‡Ùˆ Ø±Ø£ÙŠÙƒ Ø§Ù„ÙÙ†ÙŠ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØŸ"
}

field_names_ar = {
    "Date": "Ø§Ù„ØªØ§Ø±ÙŠØ®",
    "Briefing": "Ù…ÙˆØ¬Ø² Ø§Ù„ÙˆØ§Ù‚Ø¹Ø©",
    "LocationObservations": "Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹",
    "Examination": "Ù†ØªÙŠØ¬Ø© Ø§Ù„ÙØ­Øµ",
    "Outcomes": "Ø§Ù„Ù†ØªÙŠØ¬Ø©",
    "TechincalOpinion": "Ø§Ù„Ø±Ø£ÙŠ Ø§Ù„ÙÙ†ÙŠ"
}


def transcribe_audio(file_path):
    mp3_path = tempfile.mktemp(suffix=".mp3")
    ffmpeg.input(file_path).output(mp3_path).run(overwrite_output=True)
    with open(mp3_path, "rb") as f:
        transcript = client.audio.transcriptions.create(model="whisper-1", file=f)
    return transcript.text.strip()


def rephrase_text(text):
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "Ø£Ø¹Ø¯ ØµÙŠØ§ØºØ© Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ù… Ø¨Ø£Ø³Ù„ÙˆØ¨ Ø±Ø³Ù…ÙŠ Ù„ØªÙ‚Ø±ÙŠØ± ÙÙ†ÙŠ Ù„Ù„Ø´Ø±Ø·Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:"},
            {"role": "user", "content": text}
        ]
    )
    return response.choices[0].message.content.strip()


def detect_intent(text):
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "Ù…Ù‡Ù…ØªÙƒ Ø£Ù† ØªØ­Ø¯Ø¯ Ù†ÙŠØ© Ø§Ù„Ù…ØªØ­Ø¯Ø« Ø¨Ø¯Ù‚Ø©. Ø§Ø®ØªØ± ÙÙ‚Ø· ÙˆØ§Ø­Ø¯Ø© Ù…Ù†: 'approve' Ù„Ù„Ù…ÙˆØ§ÙÙ‚Ø©ØŒ 'redo' Ù„Ù„Ø¥Ø¹Ø§Ø¯Ø©ØŒ 'append' Ù„Ù„Ø¥Ø¶Ø§ÙØ© Ø£Ùˆ 'unknown' Ø¥Ù† Ù„Ù… ØªÙÙ‡Ù…."},
            {"role": "user", "content": f"Ø§Ù„Ø±Ø¯: {text}"}
        ]
    )
    return response.choices[0].message.content.strip()


def get_next_prompt(current_field):
    fields = list(field_prompts.keys())
    try:
        idx = fields.index(current_field)
        return fields[idx + 1] if idx + 1 < len(fields) else None
    except ValueError:
        return fields[0]  # Ø¨Ø¯Ø§ÙŠØ©


def speak_text(text):
    audio = generate(
        text=text,
        voice=Voice(voice_id="EXAVITQu4vr4xnSDxMaL", settings=VoiceSettings(stability=0.5, similarity_boost=0.8))
    )
    return audio


@app.route("/next", methods=["GET"])
def next_prompt():
    if not user_session["current_field"]:
        user_session["current_field"] = list(field_prompts.keys())[0]

    field = user_session["current_field"]
    prompt = field_prompts[field]
    user_session["last_prompt"] = prompt
    audio = speak_text(prompt)
    return jsonify({"prompt": prompt, "audio": base64.b64encode(audio).decode("utf-8")})


@app.route("/speak", methods=["POST"])
def process_voice():
    audio_data = request.files["audio"]
    with tempfile.NamedTemporaryFile(suffix=".webm", delete=False) as temp:
        audio_data.save(temp.name)
        temp_path = temp.name

    # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù†Øµ
    transcript = transcribe_audio(temp_path)
    intent = detect_intent(transcript)

    current_field = user_session["current_field"]

    if intent == "redo":
        response = speak_text("ğŸ” Ø­Ø³Ù†Ù‹Ø§ØŒ ÙƒØ±Ø± Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† ÙØ¶Ù„Ùƒ.")
        return jsonify({"redo": True, "audio": base64.b64encode(response).decode("utf-8")})

    elif intent == "append":
        addition = rephrase_text(transcript)
        user_session["fields"][current_field] += " " + addition
        response = speak_text("ğŸ“Œ ØªÙ…Øª Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø©ØŒ Ù‡Ù„ ØªØ±ØºØ¨ ÙÙŠ Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©ØŸ")
        return jsonify({"append": True, "audio": base64.b64encode(response).decode("utf-8")})

    elif intent == "approve" or intent == "unknown":
        refined = rephrase_text(transcript)
        user_session["fields"][current_field] = refined

        next_field = get_next_prompt(current_field)
        if next_field:
            user_session["current_field"] = next_field
            prompt = field_prompts[next_field]
            user_session["last_prompt"] = prompt
            audio = speak_text(prompt)
            return jsonify({"field_saved": True, "next_field": next_field, "audio": base64.b64encode(audio).decode("utf-8")})
        else:
            # ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡
            create_report(user_session["fields"])
            response = speak_text("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø¨Ù†Ø¬Ø§Ø­ ÙˆØªÙ… Ø¥Ø±Ø³Ø§Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ø¨Ø±ÙŠØ¯.")
            return jsonify({"done": True, "audio": base64.b64encode(response).decode("utf-8")})

    else:
        error_audio = speak_text("â— Ù„Ù… Ø£ÙÙ‡Ù…ØŒ Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªÙˆØ¶ÙŠØ­ØŸ")
        return jsonify({"error": True, "audio": base64.b64encode(error_audio).decode("utf-8")})


def create_report(fields):
    doc = DocxTemplate("police_report_template.docx")
    doc.render(fields)
    output_path = "/tmp/final_report.docx"
    doc.save(output_path)

    # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø¥ÙŠÙ…ÙŠÙ„ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ - placeholder ÙÙ‚Ø·)
    send_email(output_path)


def send_email(file_path):
    # Ø§Ø³ØªØ®Ø¯Ù… SMTP Ù„Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
    print(f"ğŸ“¤ Sending report from {file_path} to frnreports@gmail.com ... (mocked)")  # Ø¶Ø¹ Ù‡Ù†Ø§ ØªÙ†ÙÙŠØ° Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©


@app.route("/")
def index():
    return "âœ… Voice Assistant is running."


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=10000)
