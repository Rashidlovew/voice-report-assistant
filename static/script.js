let isRecording = false;
let mediaRecorder;
let audioChunks = [];
const statusText = document.getElementById("status");
const audioPlayer = document.getElementById("audioPlayer");
const transcriptionText = document.getElementById("transcriptionText");
const responseText = document.getElementById("responseText");
const fieldButtons = document.getElementById("fieldButtons");

const fields = ["Date", "Briefing", "LocationObservations", "Examination", "Outcomes", "TechincalOpinion"];
let currentFieldIndex = 0;

async function startAssistant() {
    const greeting = `ðŸ‘‹ Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨ÙƒØŒ Ù„Ù†Ø¨Ø¯Ø£ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ±. ${getPrompt(currentFieldIndex)}`;
    statusText.innerText = "ðŸ”Š ÙŠØªÙ… Ø§Ù„ØªØ´ØºÙŠÙ„...";
    await playAudioStream(greeting);
    startRecording();
}

function getPrompt(index) {
    const prompts = {
        "Date": "ðŸŽ™ï¸ Ø£Ø±Ø³Ù„ ØªØ§Ø±ÙŠØ® Ø§Ù„ÙˆØ§Ù‚Ø¹Ø©.",
        "Briefing": "ðŸŽ™ï¸ Ø£Ø±Ø³Ù„ Ù…ÙˆØ¬Ø² Ø§Ù„ÙˆØ§Ù‚Ø¹Ø©.",
        "LocationObservations": "ðŸŽ™ï¸ Ø£Ø±Ø³Ù„ Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø­ÙŠØ« Ø¨Ù…Ø¹Ø§ÙŠÙ†Ø© Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø­Ø§Ø¯Ø« ØªØ¨ÙŠÙ† Ù…Ø§ ÙŠÙ„ÙŠ .....",
        "Examination": "ðŸŽ™ï¸ Ø£Ø±Ø³Ù„ Ù†ØªÙŠØ¬Ø© Ø§Ù„ÙØ­Øµ Ø§Ù„ÙÙ†ÙŠ ... Ø­ÙŠØ« Ø¨ÙØ­Øµ Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ø­Ø§Ø¯Ø« ØªØ¨ÙŠÙ† Ù…Ø§ ÙŠÙ„ÙŠ .....",
        "Outcomes": "ðŸŽ™ï¸ Ø£Ø±Ø³Ù„ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø­ÙŠØ« Ø£Ù†Ù‡ Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§ÙŠÙ†Ø© Ùˆ Ø£Ø¬Ø±Ø§Ø¡ Ø§Ù„ÙØ­ÙˆØµ Ø§Ù„ÙÙ†ÙŠØ© Ø§Ù„Ù„Ø§Ø²Ù…Ø© ØªØ¨ÙŠÙ† Ù…Ø§ ÙŠÙ„ÙŠ:.",
        "TechincalOpinion": "ðŸŽ™ï¸ Ø£Ø±Ø³Ù„ Ø§Ù„Ø±Ø£ÙŠ Ø§Ù„ÙÙ†ÙŠ."
    };
    return prompts[fields[index]] || "";
}

async function playAudioStream(text) {
    return new Promise((resolve, reject) => {
        audioPlayer.src = `/stream-audio?text=${encodeURIComponent(text)}`;
        audioPlayer.style.display = "block";
        audioPlayer.play();
        audioPlayer.onended = () => resolve();
        audioPlayer.onerror = (err) => reject(err);
    });
}

async function startRecording() {
    if (isRecording) return;
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    mediaRecorder = new MediaRecorder(stream);
    audioChunks = [];
    mediaRecorder.ondataavailable = e => e.data.size && audioChunks.push(e.data);
    mediaRecorder.onstop = async () => {
        const audioBlob = new Blob(audioChunks, { type: "audio/webm" });
        const reader = new FileReader();
        reader.onloadend = async () => {
            const base64Audio = reader.result;
            const response = await fetch("/submitAudio", {
                method: "POST",
                headers: { "Content-Type": "application/json" },
                body: JSON.stringify({ audio: base64Audio })
            });
            const result = await response.json();
            transcriptionText.innerText = result.transcript;
            responseText.innerText = result.response;
            await playAudioStream(result.response);

            if (result.action === "redo" || result.action === "repeat") {
                startAssistant();
            } else if (result.action === "restart") {
                currentFieldIndex = 0;
                startAssistant();
            } else if (result.action === "done") {
                statusText.innerText = "âœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„ØªÙ‚Ø±ÙŠØ±";
            } else {
                currentFieldIndex++;
                if (currentFieldIndex < fields.length) {
                    startAssistant();
                } else {
                    statusText.innerText = "âœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„ØªÙ‚Ø±ÙŠØ±";
                }
            }
        };
        reader.readAsDataURL(audioBlob);
    };
    mediaRecorder.start();
    isRecording = true;
    statusText.innerText = "ðŸŽ¤ ØªØ³Ø¬ÙŠÙ„...";

    detectSilence(stream, stopRecording, 6000, 5);
}

function stopRecording() {
    if (!isRecording) return;
    isRecording = false;
    mediaRecorder.stop();
    statusText.innerText = "ðŸ›‘ ØªÙˆÙ‚Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„";
}

function detectSilence(stream, onSilence, silenceDelay = 6000, threshold = 5) {
    const audioContext = new AudioContext();
    const analyser = audioContext.createAnalyser();
    const microphone = audioContext.createMediaStreamSource(stream);
    const scriptProcessor = audioContext.createScriptProcessor(2048, 1, 1);

    analyser.smoothingTimeConstant = 0.8;
    analyser.fftSize = 2048;
    microphone.connect(analyser);
    analyser.connect(scriptProcessor);
    scriptProcessor.connect(audioContext.destination);

    let lastSoundTime = Date.now();
    scriptProcessor.onaudioprocess = () => {
        const array = new Uint8Array(analyser.fftSize);
        analyser.getByteTimeDomainData(array);
        const sum = array.reduce((acc, val) => acc + Math.pow((val - 128) / 128, 2), 0);
        const rms = Math.sqrt(sum / array.length);
        const volume = rms * 100;
        if (volume > threshold) {
            lastSoundTime = Date.now();
        }
        if (Date.now() - lastSoundTime > silenceDelay && isRecording) {
            onSilence();
            microphone.disconnect();
            scriptProcessor.disconnect();
            audioContext.close();
        }
    };
}

function renderFieldButtons() {
    fields.forEach(field => {
        const btn = document.createElement("button");
        btn.innerText = `ðŸ” Ø£Ø¹Ø¯ ${field}`;
        btn.onclick = () => {
            currentFieldIndex = fields.indexOf(field);
            startAssistant();
        };
        fieldButtons.appendChild(btn);
    });
}

window.onload = () => {
    statusText.innerText = "âœ… Ø¬Ø§Ù‡Ø²";
    renderFieldButtons();
};
